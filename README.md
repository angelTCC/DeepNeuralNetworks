# ğŸš€ Training Deep Neural Network ğŸ”„

Welcome to this deep learning fundamentals project!
Here, we focus on building a strong understanding of key DL techniques using **Multilayer Perceptrons (MLPs)** âœ…

## ğŸ¯ Project Goals

This project is all about **learning by doing**. You'll explore:

* ğŸ§  **Building MLPs from scratch** using TensorFlow/Keras
* ğŸ” **Pretraining** on one dataset (e.g., MNIST)
* ğŸ› ï¸ **Fine-tuning** the model on another (e.g., Fashion-MNIST or binary classification)
* ğŸ’¾ **Saving & loading models and weights** for transfer learning
* ğŸ§ª Playing with **hyperparameters, loss functions, optimizers**, and more

This is **not** focused on solving a specific real-world task â€” instead, itâ€™s a **concept lab** to master:

> ğŸ“ *How deep learning actually works under the hood.*

## ğŸ“‚ Project Structure

```
ğŸ“ src/
 â”£ ğŸ“„ build_model.py         # Build basic MLPs
 â”£ ğŸ“„ train_pretrain.py      # Train on dataset A (e.g., MNIST)
 â”£ ğŸ“„ fine_tune.py           # Fine-tune on dataset B (e.g., Fashion-MNIST)
 â”£ ğŸ“„ utils.py               # Utilities for preprocessing, saving, etc.

ğŸ“ data/
 â”£ ğŸ“„ download or load via TensorFlow datasets

ğŸ“ saved_models/
 â”£ ğŸ“„ pretrained_model.h5    # Example saved model
```

## ğŸš€ Getting Started

1. Clone the repository


## ğŸ“ˆ Example Results

| Phase       | Accuracy | Loss  |
| ----------- | -------- | ----- |
| Pretraining | 98.3%    | 0.045 |
| Fine-tuning | 91.7%    | 0.17  |

(*Example numbers â€“ yours will vary!*)

## ğŸ’¡ Why This Project?

This repo is perfect if you want to:

* Solidify your understanding of **neural networks** ğŸ§ 
* Practice **transfer learning & fine-tuning** fundamentals ğŸ”§
* Gain experience using **TensorFlow** and **custom training loops** ğŸ§µ

## ğŸ“š References

## ğŸ§‘â€ğŸ’» Author

Angel Chaico â€” [LinkedIn](https://www.linkedin.com) â€¢ [Twitter](https://twitter.com) â€¢ [Portfolio](#)
