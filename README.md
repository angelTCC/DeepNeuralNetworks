# 🚀 Training Deep Neural Network 🔄

Welcome to this deep learning fundamentals project!
Here, we focus on building a strong understanding of key DL techniques using **Multilayer Perceptrons (MLPs)** ✅

## 🎯 Project Goals

This project is all about **learning by doing**. You'll explore:

* 🧠 **Building MLPs from scratch** using TensorFlow/Keras
* 🔁 **Pretraining** on one dataset (e.g., MNIST)
* 🛠️ **Fine-tuning** the model on another (e.g., Fashion-MNIST or binary classification)
* 💾 **Saving & loading models and weights** for transfer learning
* 🧪 Playing with **hyperparameters, loss functions, optimizers**, and more

This is **not** focused on solving a specific real-world task — instead, it’s a **concept lab** to master:

> 🎓 *How deep learning actually works under the hood.*

## 📂 Project Structure

```
📁 src/
 ┣ 📄 build_model.py         # Build basic MLPs
 ┣ 📄 train_pretrain.py      # Train on dataset A (e.g., MNIST)
 ┣ 📄 fine_tune.py           # Fine-tune on dataset B (e.g., Fashion-MNIST)
 ┣ 📄 utils.py               # Utilities for preprocessing, saving, etc.

📁 data/
 ┣ 📄 download or load via TensorFlow datasets

📁 saved_models/
 ┣ 📄 pretrained_model.h5    # Example saved model
```

## 🚀 Getting Started

1. Clone the repository


## 📈 Example Results

| Phase       | Accuracy | Loss  |
| ----------- | -------- | ----- |
| Pretraining | 98.3%    | 0.045 |
| Fine-tuning | 91.7%    | 0.17  |

(*Example numbers – yours will vary!*)

## 💡 Why This Project?

This repo is perfect if you want to:

* Solidify your understanding of **neural networks** 🧠
* Practice **transfer learning & fine-tuning** fundamentals 🔧
* Gain experience using **TensorFlow** and **custom training loops** 🧵

## 📚 References

## 🧑‍💻 Author

Angel Chaico — [LinkedIn](https://www.linkedin.com) • [Twitter](https://twitter.com) • [Portfolio](#)
